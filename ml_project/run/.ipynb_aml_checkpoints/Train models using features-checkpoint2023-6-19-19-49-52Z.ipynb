{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Tutorial #3: Experiment and train models using features"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "In this tutorial series you will experience how features seamlessly integrates all the phases of ML lifecycle: Prototyping features, training and operationalizing.\n",
        "\n",
        "In part 1 of the tutorial you learnt how to create a feature set spec with custom transformations. In part 2 of the tutorial you learnt how to enable materialization and perform backfill. In this tutorial you will will learn how to experiment with features to improve model performance. You will see how feature store increasing agility in the experimentation and training flows. \n",
        "\n",
        "You will perform the following:\n",
        "- Prototype a create new `acccounts` feature set spec using existing precomputed values as features, unlike part 1 of the tutorial where we created feature set that had custom transformations. You will then Register the local feature set spec as a feature set in the feature store\n",
        "- Select features for the model: You will select features from the `transactions` and `accounts` feature sets and save them as a feature-retrieval spec\n",
        "- Run training pipeline that uses the Feature retrieval spec to train a new model. This pipeline will use the built in feature-retrieval component to generate the training data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Important\n",
        "\n",
        "This feature is currently in public preview. This preview version is provided without a service-level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "1. Please ensure you have executed part 1 and 2 of the tutorial"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Configure Azure ML spark notebook\n",
        "\n",
        "1. In the \"Compute\" dropdown in the top nav, select \"Serverless Spark Compute\". \n",
        "1. Click on \"configure session\" in top status bar -> click on \"Python packages\" -> click on \"upload conda file\" -> select the file azureml-examples/sdk/python/featurestore-sample/project/env/conda.yml from your local machine; Also increase the session time out (idle time) if you want to avoid running the prerequisites frequently\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Start spark session"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to start the spark session (any code block will start the session ). This can take around 10 mins.\n",
        "print(\"start spark session\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-19T17:28:37.7872576Z",
              "session_start_time": "2023-07-19T17:28:37.7884464Z",
              "execution_start_time": "2023-07-19T17:36:20.6422602Z",
              "execution_finish_time": "2023-07-19T17:36:23.7870612Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "0cf936c2-d961-4532-985b-9e02e7b3f8ba"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "start spark session\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1689788184775
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "start-spark-session",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Setup root directory for the samples"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# please update the dir to ./Users/{your-alias} (or any custom directory you uploaded the samples to).\n",
        "# You can find the name from the directory structure inm the left nav\n",
        "root_dir = \"./Users/ezzatdemnati/e2e_mlops_process\"\n",
        "\n",
        "if os.path.isdir(root_dir):\n",
        "    print(\"The folder exists.\")\n",
        "else:\n",
        "    print(\"The folder does not exist. Please create or fix the path\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-19T17:28:50.8215655Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-19T17:36:25.6292823Z",
              "execution_finish_time": "2023-07-19T17:36:26.0303233Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5f58cd08-4586-48f6-a8fc-c1194a210dd4"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The folder exists.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1689788187038
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "root-dir",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Initialize the project workspace CRUD client\n",
        "This is the current workspace where you will be running the tutorial notebook from"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize the MLClient of this project workspace\n",
        "import os\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
        "\n",
        "project_ws_sub_id = \"e0d7a68e-191f-4f51-83ce-d93995cd5c09\"\n",
        "project_ws_rg = \"my_ml_tests\"\n",
        "project_ws_name = \"myworkspace\"\n",
        "\n",
        "# connect to the project workspace\n",
        "ws_client = MLClient(\n",
        "    AzureMLOnBehalfOfCredential(), project_ws_sub_id, project_ws_rg, project_ws_name\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-19T17:29:01.702713Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-19T17:36:26.1049506Z",
              "execution_finish_time": "2023-07-19T17:36:36.5473072Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "6a46ffaf-b61d-46e0-b50a-8e6a34c66804"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1689788197542
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "init-ws-crud-client",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Initialize and get feature store \n",
        "Ensure you update the `featurestore_name` to reflect what you created in part 1 of this tutorial"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "from azure.ai.ml.entities import FeatureSetSpecification,RecurrenceTrigger\n",
        "\n",
        "import sys\n",
        "\n",
        "root_dir = \"./Users/ezzatdemnati/e2e_mlops_process\"\n",
        "sys.path.insert(0,root_dir)\n",
        "\n",
        "import featurestore.setup.featurestore_setuptools as fs_setup\n",
        "\n",
        "# Read config file:\n",
        "with open(os.path.join(root_dir,\"featurestore/config/feature_store_config.json\"),'r') as f:\n",
        "    fs_config = json.load(f)\n",
        "\n",
        "# Init FeatureStore class\n",
        "fs_class = fs_setup.FeatureStoreTools(subscription_id=fs_config[\"subscription_id\"],\n",
        "                resource_group_name=fs_config[\"resource_group_name\"],\n",
        "                location=fs_config[\"location\"],\n",
        "                featurestore_name=fs_config[\"name\"],\n",
        "                root_dir=root_dir,\n",
        "                fs_config=fs_config,\n",
        "                ml_client=None\n",
        "            )\n",
        "# Get FeatureStore\n",
        "feature_store = fs_class.get_feature_store(verbose=0)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-19T17:29:17.4549088Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-19T17:36:36.6188099Z",
              "execution_finish_time": "2023-07-19T17:36:47.006262Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3eb5a953-f560-4a2b-b70b-93c0f3186350"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root_dir:./Users/ezzatdemnati/e2e_mlops_process\nself.root_dir:./Users/ezzatdemnati/e2e_mlops_process/featurestore\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class FeatureStoreClient: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nMethod feature_stores: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n_AzureMLSparkOnBehalfOfCredential.get_token succeeded\nClass MaterializationStore: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1689788207936
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "init-fs-crud-client",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2a: Select features for model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the registered transactions feature set, version 1\r\n",
        "transactions_featureset = feature_store.feature_sets.get(\"transactions\", \"4\")\r\n",
        "accounts_featureset = feature_store.feature_sets.get(\"accounts\", \"4\")\r\n",
        "# Notice that account feature set spec is in your local dev environment (this notebook): not registered with feature store yet\r\n",
        "features = [\r\n",
        "    accounts_featureset.get_feature(\"accountAge\"),\r\n",
        "    accounts_featureset.get_feature(\"numPaymentRejects1dPerUser\"),\r\n",
        "    transactions_featureset.get_feature(\"transaction_amount_7d_sum\"),\r\n",
        "    transactions_featureset.get_feature(\"transaction_amount_3d_sum\"),\r\n",
        "    transactions_featureset.get_feature(\"transaction_amount_7d_avg\"),\r\n",
        "]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-19T17:29:21.2125007Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-19T17:36:47.0886621Z",
              "execution_finish_time": "2023-07-19T17:36:52.213299Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5c957b06-04e4-41b2-a546-544f924c8b28"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Method feature_sets: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n_AzureMLSparkOnBehalfOfCredential.get_token succeeded\n_AzureMLSparkOnBehalfOfCredential.get_token succeeded\nMethod feature_store_entities: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n_AzureMLSparkOnBehalfOfCredential.get_token succeeded\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689788213167
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 2b: Generate training data locally\n",
        "In this step we generate training data for illustrative purpose. You can optionally train models locally with this. In the upcoming steps in this tutorial, you will train a model in the cloud."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\")\r\n",
        "\r\n",
        "display(df)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2023-07-19T17:32:11.4096054Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "spark_jobs": null,
              "parent_msg_id": "1e01a835-0f48-45bf-bdb3-13e3d5f3f182"
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.featurestore import get_offline_features\r\n",
        "\r\n",
        "# Load the observation data. To understand observation data, refer to part 1 of this tutorial\r\n",
        "observation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\r\n",
        "observation_data_df = spark.read.parquet(observation_data_path)\r\n",
        "obs_data_timestamp_column = \"timestamp\"\r\n",
        "\r\n",
        "# generate training dataframe by using feature data and observation data\r\n",
        "training_df = get_offline_features(\r\n",
        "    features=features,\r\n",
        "    observation_data=observation_data_df,\r\n",
        "    timestamp_column=obs_data_timestamp_column,\r\n",
        ")\r\n",
        "\r\n",
        "# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\r\n",
        "display(training_df)\r\n",
        "# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call training_df.show() to see correctly formatted value"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "c25b756a-1f28-4821-94d2-7088ba2a2663",
              "session_id": "68",
              "statement_id": 12,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-07-19T17:30:13.3641671Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-19T17:36:52.2824711Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "collect at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/azureml/featurestore/_utils/utils.py:277",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "collect at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/azureml/featurestore/_utils/utils.py:277",
                    "description": "Job group for statement 12:\nfrom azureml.featurestore import get_offline_features\n\n# Load the observation data. To understand observation data, refer to part 1 of this tutorial\nobservation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\nobservation_data_df = spark.read.parquet(observation_data_path)\nobs_data_timestamp_column = \"timestamp\"\n\n# generate training dataframe by using feature data and observation data\ntraining_df = get_offline_features(\n    features=features,\n    observation_data=observation_data_df,\n    timestamp_column=obs_data_timestamp_column,\n)\n\n# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\ndisplay(training_df)\n# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call training_df.show() to see correctly formatted value",
                    "submissionTime": "2023-07-19T17:37:10.713GMT",
                    "stageIds": [
                      2,
                      3
                    ],
                    "jobGroup": "12",
                    "status": "RUNNING",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 0,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 0,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/azureml/featurestore/_utils/utils.py:277",
                    "dataWritten": 115,
                    "dataRead": 999413,
                    "rowCount": 114289,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "collect at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/azureml/featurestore/_utils/utils.py:277",
                    "description": "Job group for statement 12:\nfrom azureml.featurestore import get_offline_features\n\n# Load the observation data. To understand observation data, refer to part 1 of this tutorial\nobservation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\nobservation_data_df = spark.read.parquet(observation_data_path)\nobs_data_timestamp_column = \"timestamp\"\n\n# generate training dataframe by using feature data and observation data\ntraining_df = get_offline_features(\n    features=features,\n    observation_data=observation_data_df,\n    timestamp_column=obs_data_timestamp_column,\n)\n\n# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\ndisplay(training_df)\n# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call training_df.show() to see correctly formatted value",
                    "submissionTime": "2023-07-19T17:37:03.751GMT",
                    "completionTime": "2023-07-19T17:37:10.598GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\nfrom azureml.featurestore import get_offline_features\n\n# Load the observation data. To understand observation data, refer to part 1 of this tutorial\nobservation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\nobservation_data_df = spark.read.parquet(observation_data_path)\nobs_data_timestamp_column = \"timestamp\"\n\n# generate training dataframe by using feature data and observation data\ntraining_df = get_offline_features(\n    features=features,\n    observation_data=observation_data_df,\n    timestamp_column=obs_data_timestamp_column,\n)\n\n# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\ndisplay(training_df)\n# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call training_df.show() to see correctly formatted value",
                    "submissionTime": "2023-07-19T17:36:53.443GMT",
                    "completionTime": "2023-07-19T17:36:59.993GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "8ee6ea59-b748-493b-95dc-347c10c902e7"
            },
            "text/plain": "StatementMeta(c25b756a-1f28-4821-94d2-7088ba2a2663, 68, 12, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 2c: Register the `accounts` featureset with the featurestore\n",
        "Once you have experimented with different feature definitions locally and sanity tested it, you can register it with the feature store.\n",
        "For this you will register a featureset asset definition with the feature store.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import FeatureSet, FeatureSetSpecification\n",
        "\n",
        "accounts_fset_config = FeatureSet(\n",
        "    name=\"accounts\",\n",
        "    version=\"1\",\n",
        "    description=\"accounts featureset\",\n",
        "    entities=[\"azureml:account:1\"],\n",
        "    stage=\"Development\",\n",
        "    specification=FeatureSetSpecification(path=accounts_featureset_spec_folder),\n",
        "    tags={\"data_type\": \"nonPII\"},\n",
        ")\n",
        "\n",
        "poller = fs_client.feature_sets.begin_create_or_update(accounts_fset_config)\n",
        "print(poller.result())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683424692142
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "reg-accts-fset",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 2d: Get registered featureset and sanity test"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look up the featureset by providing name and version\n",
        "accounts_featureset = featurestore.feature_sets.get(\"accounts\", \"1\")\n",
        "# get access to the feature data\n",
        "accounts_feature_df = accounts_featureset.to_spark_dataframe()\n",
        "display(accounts_feature_df.head(5))\n",
        "# Note: Please ignore this warning: Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683424718158
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "sample-accts-fset-data",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run training experiment\n",
        "In this step you will select a list of features, run a training pipeline, and register the model. You can repeat this step till you are happy with the model performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### (Optional) Step 3a: Discover features from Feature Store UI\n",
        "You have already done this in part 1 of the tutorial after registering the `transactions` feature set. Since you also have `accounts` featureset, you can browse the available features:\n",
        "* Goto the [Azure ML global landing page](https://ml.azure.com/home?flight=FeatureStores).\n",
        "* Click on `Feature stores` in the left nav\n",
        "* You will see the list of feature stores that you have access to. Click on the feature store that you created above.\n",
        "\n",
        "You can see the feature sets and entity that you created. Click on the feature sets to browse the feature definitions. You can also search for feature  sets across feature stores by using the global search box."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### (Optional) Step 3b: Discover features from SDK"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available feature sets\n",
        "all_featuresets = featurestore.feature_sets.list()\n",
        "for fs in all_featuresets:\n",
        "    print(fs)\n",
        "\n",
        "# List of versions for transactions feature set\n",
        "all_transactions_featureset_versions = featurestore.feature_sets.list(\n",
        "    name=\"transactions\"\n",
        ")\n",
        "for fs in all_transactions_featureset_versions:\n",
        "    print(fs)\n",
        "\n",
        "# See properties of the transactions featureset including list of features\n",
        "featurestore.feature_sets.get(name=\"transactions\", version=\"1\").features"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683424900729
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "discover-features-from-sdk",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 3c: Select features for the model and export it as a feature-retrieval spec\n",
        "In the previous steps, you selected features from a combination unregistered  and registered feature sets for local experimentation and testing. Now you are ready to experiment in the cloud. Saving the selected features as a feature-retrieval spec and using it in the mlops/cicd flow for training/inference increases your agility in shipping models."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Select features for the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can select features in pythonic way\n",
        "features = [\n",
        "    accounts_featureset.get_feature(\"accountAge\"),\n",
        "    transactions_featureset.get_feature(\"transaction_amount_7d_sum\"),\n",
        "    transactions_featureset.get_feature(\"transaction_amount_3d_sum\"),\n",
        "]\n",
        "\n",
        "# you can also specify features in string form: featurestore:featureset:version:feature\n",
        "more_features = [\n",
        "    \"accounts:1:numPaymentRejects1dPerUser\",\n",
        "    \"transactions:1:transaction_amount_7d_avg\",\n",
        "]\n",
        "\n",
        "more_features = featurestore.resolve_feature_uri(more_features)\n",
        "\n",
        "features.extend(more_features)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683424978050
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "select-reg-features",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Export selected features as a feature-retrieval spec\n",
        "\n",
        "#### Note\n",
        "Feature retrieval spec is a portable definition of list of features associated with a model. This can help streamline ML model development and operationalizing.This will be an input to the training pipeline (used to generate the training data), then will be packaged along with the model, and will be used during inference to lookup the features. It will be a glue that integrates all phases of the ML lifecycle. Changes to training/inference pipeline can be kept minimal as you experiment and deploy. \n",
        "\n",
        "Using feature retrieval spec and the built-in feature retrieval component is optional: you can directly use `get_offline_features()` api as shown above.\n",
        "\n",
        "Note that the name of the spec should be `feature_retrieval_spec.yaml` when it is packaged with the model for the system to recognize it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature retrieval spec\n",
        "feature_retrieval_spec_folder = root_dir + \"/project/fraud_model/feature_retrieval_spec\"\n",
        "\n",
        "# check if the folder exists, create one if not\n",
        "if not os.path.exists(feature_retrieval_spec_folder):\n",
        "    os.makedirs(feature_retrieval_spec_folder)\n",
        "\n",
        "featurestore.generate_feature_retrieval_spec(feature_retrieval_spec_folder, features)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683425334483
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "export-as-frspec",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train in the cloud using pipelines and register model if satisfactory\n",
        "In this step you will manually trigger the training pipeline. In a production scenario, this could be triggered by a ci/cd pipeline based on changes to the feature-retrieval spec in the source repository."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 4a: Run the training pipeline\n",
        "The training pipeline has the following steps:\n",
        "\n",
        "1. Feature retrieval step: This is a built-in component takes as input the feature retrieval spec, the observation data and timestamp column name. It then generates the training data as output. It runs this as a managed spark job.\n",
        "1. Training step: This step trains the model based on the training data and generates a model (not registered yet)\n",
        "1. Evaluation step: This step validates whether model performance/quailty is within threshold (in our case it is a placeholder/dummy step for illustration purpose)\n",
        "1. Register model step: This step registers the model\n",
        "\n",
        "Note: In part 2 of this tutorial you ran a backfill job to materialize data for `transactions` feature set. Feature retrieval step will read feature values from offline store for this feature set. The behavior will same even if you use `get_offline_features()` api."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_job  # will be used later\n",
        "\n",
        "training_pipeline_path = (\n",
        "    root_dir + \"/project/fraud_model/pipelines/training_pipeline.yaml\"\n",
        ")\n",
        "training_pipeline_definition = load_job(source=training_pipeline_path)\n",
        "training_pipeline_job = ws_client.jobs.create_or_update(training_pipeline_definition)\n",
        "ws_client.jobs.stream(training_pipeline_job.name)\n",
        "# Note: First time it runs, each step in pipeline can take ~ 15 mins. However subsequent runs can be faster (assuming spark pool is warm - default timeout is 30 mins)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683429020656
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "run-training-pipeline",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Inspect the training pipeline and the model\n",
        "Open the above pipeline run \"web view\" in new window to see the steps in the pipeline.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 4b: Notice the feature retrieval spec in the model artifacts\n",
        "1. In the left nav of the current workspace -> right click on `Models` -> Open in new tab or window\n",
        "1. Click on `fraud_model`\n",
        "1. Click on `Artifacts` in the top nav\n",
        "\n",
        "You can notice that the feature retrieval spec is packaged along with the model. The model registration step in the training pipeline has done this. You created feature retrieval spec during experimentation, now it has become part of the model definition. In the next tutorial you will see how this will be used during inferencing.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Step 5: View the feature set and model dependencies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 5a: View the list of feature sets associated with the model\n",
        "In the same models page, click on the `feature sets` tab. Here you can see both `transactions` and `accounts` featuresets that this model depends on."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 5b: View the list of models using the feature sets\n",
        "1. Open the feature store UI (expalined in a previous step in this tutorial)\n",
        "1. Click on `Feature sets` on the left nav\n",
        "1. Click on any of the feature set -> click on `Models` tab\n",
        "\n",
        "You can see the list of models that are using the feature sets (determined from the feature retrieval spec when the model was registered)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Cleanup\n",
        "\n",
        "Part 4 of the tutorial has instructions for deleting the resources"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "* Part 4 of tutorial: Enable recurrent materialization and run batch inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}